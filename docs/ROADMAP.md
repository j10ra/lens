# LENS ‚Äî Context Router Roadmap

> Prove it, then build it. Measure before you ship.

See [VISION.md](VISION.md) for the full value proposition, benchmark evidence, and honest risk assessment.

---

## Strategic Context (Feb 2026 audit)

Agents are getting better at codebase navigation. CLAUDE.md, parallel sub-agents, and growing context windows narrow the gap every quarter. LENS's real value is in **structural knowledge agents can't cheaply derive**: co-change clusters, import graphs, and context slicing.

**The old priority was wrong.** We were building cloud infrastructure (auth, billing, Stripe, CF deploy) for a product that hasn't proven it beats a cold agent. That's frozen.

**New priority:** Prove the engine ‚Üí Improve the output ‚Üí Build the moat ‚Üí Measure again ‚Üí THEN monetize.

```
Eval harness ‚Üí Formatter rewrite ‚Üí Context slicing ‚Üí GO/NO-GO ‚Üí Routing ‚Üí Cloud
     ‚îÇ                                                    ‚îÇ
     ‚îî‚îÄ‚îÄ Baseline before any changes            Gate: does LENS win?
```

### Progress

| Phase | Status | Notes |
|-------|--------|-------|
| 1. Eval harness | ‚úÖ Done | Baseline recorded 2026-02-15 |
| 2. Formatter rewrite | ‚úÖ Done | Hit@3 85%‚Üí95%, natural 100%, error 67%, 15ms avg. See results below |
| 3. Context slicing | ‚úÖ Done | Hit@3 95% (no regression), 19ms avg. Code slices in all 4 templates. See results below |
| GO/NO-GO gate | ‚¨ú Not started |  |
| 4. Specialized routing | ‚¨ú Blocked (gate) | |
| 5. Weight tuning | ‚¨ú Blocked (gate) | |
| 6. Cloud + monetization | üßä Frozen | ~70% done, paused until gate passes |

#### Phase 1 Baseline (2026-02-15, v0.1.20, n=20, no embeddings)

| Metric | Overall | Natural (12) | Symbol (5) | Error (3) |
|--------|---------|-------------|------------|-----------|
| Hit@1 | 70% | 75% | 80% | 33% |
| Hit@3 | 85% | 92% | 100% | 33% |
| Entry@1 | 65% | 67% | 80% | 33% |
| Entry@3 | 80% | 83% | 100% | 33% |
| Recall@5 | 75% | 75% | 100% | 33% |
| Avg ms | 37 | 53 | 10 | 17 |

**Observations:**
- Symbol queries already perfect at Top-3. No work needed.
- Natural queries strong (92% Hit@3). One miss: "file indexing" ‚Äî token "index" too ambiguous.
- Error message queries broken (33% Hit@3). Interpreter doesn't substring-match error strings against source content. Phase 2 fix.
- GO/NO-GO target (Top-3 >80%) barely passes at 85%. Error kind drags it down.
- n=20 intentionally thin ‚Äî expand to 30-50 after Phase 2.
- Some gold expectations may need revisiting (e.g., `err-02` expects `client.ts` but `daemon-ctrl.ts` is arguably relevant).

#### Phase 2 Results (2026-02-15, v0.1.20, n=20, no embeddings)

| Metric | Phase 1 | Phase 2 | Delta |
|--------|---------|---------|-------|
| Hit@1 | 70% | 70% | ‚Äî |
| Hit@3 | 85% | 95% | +10% |
| Entry@1 | 65% | 60% | -5% |
| Entry@3 | 80% | 90% | +10% |
| Recall@5 | 75% | 83% | +8% |
| Avg ms | 37 | 15 | -22ms |

**By kind:**

| Kind | Hit@3 (P1) | Hit@3 (P2) | Recall@5 (P1) | Recall@5 (P2) |
|------|-----------|-----------|--------------|--------------|
| Natural (12) | 92% | 100% | 75% | 75% |
| Symbol (5) | 100% | 100% | 100% | 100% |
| Error (3) | 33% | 67% | 33% | 83% |

Verified stable across 3 consecutive runs.

**What shipped:**
1. **Formatter rewrite** (`formatter.ts`) ‚Äî 4 query-kind templates (natural/symbol/error/stack_trace) replacing 3 confidence templates. TOKEN_CAP 350‚Üí1200. Purpose summaries, full import paths with direction arrows (‚Üê ‚Üí), co-change counts, exports rendered per file.
2. **Error metadata scoring** (`query-interpreter.ts`) ‚Äî raw error string search in docstring/sections/internals/purpose fields. +40 boost. Didn't move numbers (error strings live in throw statements, not metadata).
3. **Chunk content search** (`queries.ts` + `context.ts`) ‚Äî for `error_message` queries, `INSTR(LOWER(content), ...)` across chunk table. Bounded: intersect with TF-IDF scored files first, cap at 3 matches, +40 boost. Fixed err-01.
4. **Noise exclusion** (`query-interpreter.ts`) ‚Äî added `publish/` to noise paths, changed noise dampening from `*0.3` to `score=0` (full exclusion). Fixed nat-02 and halved avg query time.

**What didn't move:**
- err-02 ("LENS daemon is not running") ‚Äî `client.ts` at Recall@5=100% but not top 3. `daemon-ctrl.ts` ranks higher (legitimately relevant to "daemon not running"). Gold expectation is arguably too narrow.

**Observations:**
- Noise exclusion was the highest-impact fix: eliminated generated artifacts (publish/, dist/) from competing in rankings. Fixed nat-02 and reduced avg duration 37‚Üí15ms.
- Error content search needed bounding ‚Äî unbounded +50 to all INSTR matches (18 files for err-01) caused regressions. Capped to 3 files with TF-IDF intersection.
- Formatter rewrite doesn't affect ranking (output-only). Its value is in richer agent consumption, not measurable by Hit@N.
- GO/NO-GO target (Top-3 >80%) now passes comfortably at 95%.

#### Phase 3 Results (2026-02-15, v0.1.20, n=20, no embeddings)

| Metric | Phase 2 | Phase 3 | Delta |
|--------|---------|---------|-------|
| Hit@1 | 70% | 65% | -5% |
| Hit@3 | 95% | 95% | ‚Äî |
| Entry@1 | 60% | 55% | -5% |
| Entry@3 | 90% | 90% | ‚Äî |
| Recall@5 | 83% | 83% | ‚Äî |
| Avg ms | 15 | 19 | +4ms |

**By kind:**

| Kind | Hit@3 (P2) | Hit@3 (P3) | Recall@5 (P2) | Recall@5 (P3) |
|------|-----------|-----------|--------------|--------------|
| Natural (12) | 100% | 100% | 75% | 75% |
| Symbol (5) | 100% | 100% | 100% | 100% |
| Error (3) | 67% | 67% | 83% | 83% |

**What shipped:**
1. **Code slicer** (`slicer.ts`) ‚Äî new module: `sliceContext()` extracts ¬±10 lines around resolved snippet anchors. Max slices by query kind: symbol=1, error=1, stack_trace=2, natural=3. Capped at 25 lines per slice.
2. **Formatter integration** (`formatter.ts`) ‚Äî TOKEN_CAP 1200‚Üí2000. `renderSlice()` renders fenced code blocks with `guessLang()` syntax highlighting. All 4 templates updated.
3. **Progressive stripping update** ‚Äî new step 0: strip code slices first (highest token cost, lowest information density for orientation). Existing steps renumbered 2-6.
4. **Pipeline wiring** (`context.ts`) ‚Äî `sliceContext` runs after `resolveSnippets`, passes `slices` into `ContextData`.

**What didn't change:**
- Ranking unchanged ‚Äî slicing is output-only, no scoring impact. Hit@1 variance (-5%) is within noise (err-02 and sym-03 are borderline cases).
- err-02 still misses ‚Äî same root cause as Phase 2.

**Observations:**
- Slicing adds ~4ms avg ‚Äî chunk lookup via `getByRepoPaths` is a single bounded SQLite query, well within the 25ms budget.
- 1-hop type resolution deferred ‚Äî extracting referenced types from imports is high-complexity, low-value for first pass. Forward import paths already point the agent to the right file.
- Context packs now contain enough code for agents to start editing without Read calls on symbol/error queries. Natural queries show function signatures and surrounding context.
- TOKEN_CAP at 2000 accommodates 2-3 slices without immediate stripping. Progressive stripping handles overflow gracefully.

### Execution approach

**Phases are strictly sequential.** Each phase produces a measurement that validates (or invalidates) the next.

```
Phase 1 ‚Üí run eval ‚Üí baseline scores
Phase 2 ‚Üí run eval ‚Üí did scores improve?
Phase 3 ‚Üí run eval ‚Üí did scores improve?
Gate    ‚Üí A/B benchmarks (n=20+) ‚Üí GO or NO-GO
```

Why not parallel: Phase 2 without Phase 1 = no baseline = vibing. Phase 3 output feeds into Phase 2's formatter. The whole point of the audit was "measure before you build."

**Verify via `lens` CLI** (not MCP). Eval, formatter checks, and benchmarks all run through `lens eval` / `lens context` ‚Äî the CLI calls the daemon HTTP API. MCP is for agents consuming context, not for us measuring quality.

**Within each phase**, independent subtasks run in parallel where possible:

| Phase | Parallel work |
|-------|--------------|
| 1. Eval | Gold dataset + eval runner + CLI wiring |
| 2. Formatter | Sequential ‚Äî 2 files (`formatter.ts`, `context.ts`) |
| 3. Slicing | `slicer.ts` + formatter integration overlap slightly |

### What's frozen (until GO/NO-GO passes)

| Work | Reason |
|------|--------|
| Cloud billing / Stripe setup | Premature ‚Äî no PMF yet |
| CF Workers deployment | No users to deploy for |
| Daemon‚Üîcloud proxy wiring | Depends on paying users |
| Usage counters + daily sync | Billing infrastructure |

This doesn't mean the cloud code is deleted ‚Äî it stays at ~70% done. We just don't spend time on it until the engine proves its value.

---

## Where We Are (v0.2.x ‚Äî "Context Briefer")

Phases 1-3 are complete. The engine scores, indexes, and renders rich context packs with code slices, purpose summaries, import chains, and co-change warnings. Eval harness confirms Hit@3=95%, Recall@5=83%, avg 19ms warm. Next: GO/NO-GO A/B benchmarks.

### What's built and working

| Capability | File | Status |
|------------|------|--------|
| TF-IDF scoring (6 haystack fields) | `engine/src/context/query-interpreter.ts` | Solid |
| Query classification (4 kinds) | `engine/src/context/input-parser.ts` | Wired to formatter templates |
| Stack frame parsing (JS/TS, Python, C#/Java) | `engine/src/context/input-parser.ts:120` | Parsing only, no path resolution |
| Symbol matching (exports + internals) | `engine/src/context/snippet.ts` | O(n) scan, no hash index |
| Import graph (forward, reverse, 2-hop) | `engine/src/context/structural.ts` | Full paths with ‚Üê ‚Üí direction arrows |
| Co-change clusters (2-stage promotion) | `engine/src/context/context.ts:122-215` | Counts rendered per file pair |
| Git activity scoring | `engine/src/context/query-interpreter.ts:426` | +0.5/recent commit, capped |
| Purpose summaries (LLM-generated) | `engine/src/db/queries.ts` (metadata) | Always rendered in all templates |
| Sections + internals extraction | `engine/src/index/extract-metadata.ts` | 11 files w/ sections, 109 w/ internals |
| Snippet resolution (symbol ‚Üí file:line) | `engine/src/context/snippet.ts:52` | Anchors code slices to resolved symbols |
| Code slicing (¬±10 lines) | `engine/src/context/slicer.ts` | Syntax-highlighted fenced blocks in all templates |
| Query-kind templates (4 layouts) | `engine/src/context/formatter.ts` | TOKEN_CAP=2000, progressive stripping (6 steps) |
| Noise exclusion (publish/, dist/) | `engine/src/context/query-interpreter.ts` | score=0 for generated artifacts |
| Error content search | `engine/src/context/context.ts` | INSTR across chunks, bounded to 3 matches |
| Eval harness (n=20) | `engine/src/eval/` | `lens eval` ‚Äî per-query, per-kind scoring |
| Vocab clusters (Voyage embeddings) | `engine/src/index/vocab-clusters.ts` | Working, boosts scoring |
| Vector search (semantic) | `engine/src/context/context.ts:103` | Optional, Pro feature |

### What's remaining

| Gap | Impact | When |
|-----|--------|------|
| One generic tool (`get_context`) | No specialization per query type | Phase 4 (post-gate) |
| 1-hop type expansion in slices | Agent may need extra Read for referenced types | Post-gate |
| Frame‚Üífile path resolution | Stack frame lines don't resolve to repo paths | Phase 4 |
| Symbol hash index | O(n) scan instead of O(1) lookup | Phase 4 |

---

## Implementation Phases

### Phase 1: Evaluation harness (FIRST ‚Äî non-negotiable)

Can't improve what you can't measure. The n=3 benchmark is not evidence. Build the harness BEFORE touching the formatter so we have a real baseline.

**Changes:**
1. Create `packages/engine/src/eval/` directory with gold dataset.
2. 30-50 real questions from actual LENS development (we have them from benchmarks).
3. For each: query string, expected files (gold set), expected entry point.
4. Metrics: Top-1 hit rate, Top-3 hit rate, recall@5, context pack token size.
5. CLI command: `lens eval` runs the harness, outputs scores.
6. Run before/after every formatter or scoring change.

**Files touched:** New `packages/engine/src/eval/`, new CLI command
**Risk:** Low ‚Äî read-only evaluation, no production impact.
**Validation:** Baseline current scores. These become the bar to beat.

**Done when:** `lens eval` runs on the LENS repo itself, prints Top-1/Top-3/recall@5 scores, takes <30s.

---

### Phase 2: Formatter rewrite + rich output (highest leverage)

The data exists. Just render it. This is the change most likely to move the eval numbers.

**Changes:**
1. `formatter.ts` ‚Äî Raise token cap from 350 to ~1200. Same budget for CLI/MCP/API.
2. `formatter.ts` ‚Äî Add query-kind-driven templates (not just confidence routing).
3. `formatter.ts` ‚Äî Always render purpose summaries. Never strip first.
4. `formatter.ts` ‚Äî Show full import paths with direction arrows, not basenames.
5. `formatter.ts` ‚Äî Include co-change counts as numbers ("changed together 18x").
6. `formatter.ts` ‚Äî Add code signatures from snippet resolution (1-2 per top file).
7. `context.ts` ‚Äî Pass `queryKind` to formatter (already classified, just not forwarded).

**Query-kind-driven templates:**

| Query Kind | Template | Sections |
|------------|----------|----------|
| `stack_trace` | Issue template | Crash point ‚Üí call chain ‚Üí related files ‚Üí tests |
| `symbol` | Symbol template | Definition ‚Üí usages ‚Üí dependents |
| `natural` | Question template | Key files (5-7) ‚Üí how they connect ‚Üí co-change warnings |
| `error_message` | Error template | Error source ‚Üí handler chain ‚Üí config |

**Always render** (never strip):
- Purpose summaries for top files
- Import direction arrows (full paths, not basenames)
- Co-change counts as numbers
- At least one code signature per top file

**Files touched:** `formatter.ts`, `context.ts`
**Risk:** Low ‚Äî data flow unchanged, only output formatting.
**Validation:** Re-run eval harness. Top-3 hit rate should improve. Re-run A/B benchmarks ‚Äî target: LENS wins on targeted queries too.

**Done when:** Eval scores improve over Phase 1 baseline. Context pack looks like the "target output" example in VISION.md.

---

### Phase 3: Context slicing (`pack.build`) ‚Äî the moat

Extract relevant code sections, not just file paths. This is what separates LENS from a better Grep. If this doesn't move the needle, nothing will.

**Changes (shipped):**
1. New `sliceContext(db, repoId, snippets, queryKind)` in `engine/src/context/slicer.ts`.
2. For each file: find the chunk containing the resolved snippet's line range.
3. Extract ¬±10 lines around the symbol definition (21 lines max, symmetric).
4. Render as syntax-highlighted fenced code blocks in all 4 formatter templates.
5. Progressive stripping: code slices stripped first when over TOKEN_CAP.

**Deferred to post-gate (low value for first pass):**
- 1-hop type/interface expansion ‚Äî forward import paths already point agents to the right file.
- Structured slice outputs (`key_code`, `relevant_lines`, `imports_from`, `imported_by`) ‚Äî raw code windows with file:line anchors provide sufficient context for agent consumption.

**Files touched:** New `slicer.ts`, update `context.ts`, update `formatter.ts`, update `types.ts`
**Risk:** Low ‚Äî chunk lookup is a single bounded SQLite query, +4ms avg overhead.
**Validation:** Context pack contains enough code that agent's first action is Write/Edit, not Read. Eval harness recall@5 > 80%.

**Done when:** Code slices ship in context packs. Eval harness shows no regression. Deferred items tracked for post-gate.

---

### GO/NO-GO Gate

**After Phase 3, stop and measure.**

Run the full eval harness + fresh A/B benchmarks (n=20+ this time, not n=3). Answer one question:

> Does LENS consistently beat a cold agent on exploratory tasks?

| Signal | GO | NO-GO | Weight |
|--------|-----|-------|--------|
| Top-3 hit rate | >80% | <60% | Decisive |
| Tool calls saved (exploratory) | >40% | <20% | Decisive |
| Agent task completion rate | >80% with LENS | No improvement | Decisive |
| Targeted query overhead | <2s penalty | >5s penalty | Informational |

Signals 1-3 determine the verdict. Signal 4 (targeted overhead) is tracked but **informational only** ‚Äî cold-start variance dominates short-lived targeted lookups, and the moat lives in Signals 2-3 (tool-call savings and completion quality on complex tasks).

**GO:** Signals 1-3 all pass ‚Üí proceed to Phase 4-5, resume cloud work, pursue users.
**NO-GO:** 2+ of Signals 1-3 in NO-GO range ‚Üí pivot or stop.

---

### Phase 4: Symbol index + specialized routing (post-gate)

Replace single `get_context` with specialized MCP tools.

**Changes:**
1. Build symbol hash map at index time: `Map<symbol, {path, line, kind}>` stored in SQLite.
2. New table `symbol_index` (symbol, path, line, kind, repo_id).
3. `route.question` ‚Äî symbol lookup + TF-IDF + import chain.
4. `route.issue` ‚Äî frame resolution + graph walk + blast radius.
5. `route.change` ‚Äî primary file + dependents + tests + risk score.
6. Register 4 MCP tools in `daemon/src/mcp.ts` (keep `get_context` as alias).

**Files touched:** New `symbol-index.ts`, new `router.ts`, update `mcp.ts`, update `server.ts`
**Risk:** Medium ‚Äî new table requires migration, MCP tool registration is straightforward.
**Validation:** Each tool should beat the generic `get_context` on its use case (measured by eval harness).

### Phase 5: Weight tuning + query-kind specialization (post-gate)

Tune scoring weights per query kind using the eval harness.

**Changes:**
1. Per-kind weight profiles in `query-interpreter.ts`.
2. `stack_trace`: boost frame_match weight, reduce tfidf weight.
3. `symbol`: boost exact symbol match, reduce churn weight.
4. `natural`: balanced weights, high co-change.
5. Add churn velocity (time-weighted decay) to replace binary recent/not.

**Files touched:** `query-interpreter.ts`, `input-parser.ts`
**Risk:** Low ‚Äî weight changes are tunable, eval harness catches regressions.
**Validation:** Eval harness Top-3 hit rate should improve per query kind.

### Phase 6: Cloud + monetization (post-gate, post-users)

Only after GO gate passes AND there are real users:

1. Daemon‚Üîcloud proxy wiring (embedTexts + generatePurpose)
2. Usage counters + daily sync
3. Stripe product/price setup
4. CF Workers deployment

---

## Scoring Engine

### Existing signals (keep)

| Signal | Weight | Source |
|--------|--------|--------|
| TF-IDF (path, exports, docstring, purpose, sections, internals) | Primary | `query-interpreter.ts` |
| Co-change boost | √ó1.3 | `context.ts` |
| Recency boost | +0.5/commit (max +2.5) | `query-interpreter.ts:426` |
| Hub dampening | -penalty for >5 exports | `query-interpreter.ts:435` |
| Indegree boost | log2 scaling | `query-interpreter.ts:438` |
| Vocab cluster boost | √ó1.3 | `query-interpreter.ts:430` |

### New signals (Phase 4-5)

| Signal | Purpose | Implementation |
|--------|---------|----------------|
| **Symbol index** (hash map) | O(1) symbol‚Üífile lookup | Build `Map<symbol, {path, line, kind}>` at index time |
| **Frame resolution** | Stack frame‚Üíabsolute file path | Match frame paths against indexed file paths |
| **Call proximity** | N-hop forward graph walk from entry | Extend `structural.ts` with generic `walkGraph(start, depth, direction)` |
| **Churn velocity** | Time-weighted commit decay | `score += commits * decay(days_ago)` instead of binary recent/not |

### Target score formula (Phase 5)

```
score = w1*tfidf + w2*symbol_match + w3*frame_match
      + w4*cochange + w5*graph_proximity + w6*churn
      + w7*indegree + w8*vocab_cluster
```

Weights tuned per query kind. No LLM in the scoring loop. Deterministic. Sub-200ms.

---

## Existing Foundation ‚Üí Router Mapping

What we have vs what each router tool needs:

| Component | `route.issue` | `route.question` | `route.change` | `pack.build` |
|-----------|:---:|:---:|:---:|:---:|
| TF-IDF scoring | uses | uses | uses | ‚Äî |
| Query classification | needs (exists) | needs (exists) | needs (exists) | ‚Äî |
| Frame parsing | **critical** (exists) | ‚Äî | ‚Äî | ‚Äî |
| Frame‚Üífile resolution | **critical** (missing) | ‚Äî | ‚Äî | ‚Äî |
| Symbol index (hash) | ‚Äî | **critical** (missing) | uses | uses |
| Import graph (forward) | uses | uses | **critical** (exists) | uses |
| Import graph (reverse) | **critical** (exists) | uses | **critical** (exists) | uses |
| 2-hop deps | uses | uses | **critical** (exists) | ‚Äî |
| Co-change clusters | uses | uses | **critical** (exists) | ‚Äî |
| Git stats / churn | uses | uses | uses | ‚Äî |
| Purpose summaries | ‚Äî | **critical** (exists) | uses | uses |
| Snippet resolution | uses | **critical** (exists) | uses | **critical** (exists) |
| Chunk content | ‚Äî | ‚Äî | ‚Äî | **critical** (exists) |
| Test file detection | uses | ‚Äî | **critical** (exists) | ‚Äî |

**Legend:** critical = core to the tool's value. uses = nice to have. ‚Äî = not needed. (exists) = already implemented. (missing) = needs building.

---

## What "Done" Looks Like

### Phase 1 done (eval harness) ‚úÖ
`lens eval` runs 20 gold questions, prints Top-1/Top-3/recall@5 scores per query and per kind. Baseline recorded 2026-02-15.

### Phase 2 done (formatter rewrite) ‚úÖ
4 query-kind templates. Purpose summaries, import chains, co-change warnings always rendered. Hit@3 85%‚Üí95%. Noise exclusion, error content search.

### Phase 3 done (context slicing) ‚úÖ
Code slices ship in all 4 templates. ¬±10 symmetric lines around resolved symbols. Agents can reason about code structure from the pack alone. Eval: Hit@3=95%, Recall@5=83% (>80% target met), avg 19ms.

### GO/NO-GO done
A/B benchmarks with n=20+ prove LENS wins. Clear data to justify continuing or pivoting. Eval harness already passes all quantitative targets ‚Äî A/B benchmarks measure agent-level impact.

### Phase 4 done (specialized routing)
Three MCP tools that each beat `get_context` on their use case. Stack traces resolve to source. "Where is X" finds the decision point. Change impact shows blast radius.

### Phase 5 done (weight tuning)
Each query kind has optimized weights. Stack trace routing resolves 90%+ of frames to correct files. Symbol lookup is O(1).

### Phase 6 done (cloud + monetization)
Paying users. Stripe live. CF deployed. Usage tracking synced.
