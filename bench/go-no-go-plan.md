# GO/NO-GO Gate — Test Plan

## 1. Objective

Determine whether LENS (Local-First Repo Context Engine) provides measurable value as an **on-demand MCP tool** that AI coding agents can call when needed, compared to working cold (no context assistance).

**Decision**: Should development continue past Phase 3 (engine proof) into Phase 4 (routing) and beyond?

---

## 2. What is Being Tested

**LENS as an MCP tool** — LENS tools (`get_context`, `list_repos`, `get_status`, `index_repo`) are made available via `--mcp-config`. The agent decides when/if to call them. No pre-injected context, no forced usage. This tests the product-realistic mode where agents pull context on-demand.

**Two conditions per task:**

| Condition | What the agent gets | Disallowed tools | LENS MCP |
|-----------|-------------------|-----------------|----------|
| **WITHOUT** | Raw task prompt only | Task + all 4 LENS MCP tools | Unavailable |
| **WITH** | Raw task prompt only (identical) | Task only | Available via `--mcp-config` |

Both conditions use the same model, same environment, same prompt. The **only variable** is whether LENS MCP tools (`get_context`, `list_repos`, `get_status`, `index_repo`) are available for the agent to call on-demand.

---

## 3. Test Subjects

Two codebases of different size and domain:

| Repo | Language | Domain | Size |
|------|----------|--------|------|
| LENS (RLM) | TypeScript | Monorepo — engine, daemon, CLI, cloud | ~150 files |
| Pinnacle | C# / React | Enterprise — container logistics, billing, auth | ~2000+ files |

Testing on two repos avoids overfitting to the repo LENS was developed against.

---

## 4. Scenario Generation

For each repo, the `/benchmark` skill generates test scenarios across 4 categories. Count is configurable via `-n` flag (default: 8):

| Category | Purpose | Expected LENS advantage |
|----------|---------|------------------------|
| **Exploratory** | Trace architecture, map workflows | HIGH — context reduces blind search |
| **Debug** | Diagnose errors, trace failure paths | MEDIUM — context narrows scope |
| **Change-Impact** | Map files affected by a change | MEDIUM — co-change data helps |
| **Targeted** | Find a specific constant/class/config | LOW — simple grep often enough |

Scenarios are generated by a separate AI instance analyzing the repo. Each scenario includes:
- A realistic prompt an engineer would ask
- 3-5 **judge criteria** — verifiable facts the answer must mention

**Total expected**: configurable. For gate-level sample size, use `-n 12` per repo = 24 tasks (48 runs).

---

## 5. Execution Method

### Model

All runs use the same model: **glm-5** via Z.AI API routing. No mixing of models between conditions.

### Runner

Each task is executed by `claude -p` (pipe mode) — a fresh, stateless AI session with no history. The runner script (`.claude/skills/benchmark/run.sh`) handles:

- **Z.AI routing** — isolated `( ... )` subshell; reads `$GLOBAL_GLM_KEY` from environment (no hardcoded secrets)
- **Randomized order** — within each task, which condition runs first is randomized (`$RANDOM`) to eliminate order bias
- **Sequential execution** — the two conditions for a task run one after the other (not parallel) to avoid resource contention
- **Retries** — up to 3 attempts if output is empty
- **Timing** — wall-clock duration recorded in `timing.csv`
- **Skip existing** — completed runs are not re-run

### Controls

| Control | Implementation |
|---------|---------------|
| Same model both conditions | `--model sonnet` (mapped to glm-5 via env vars). Configured model mapping logged in `timing.csv`. |
| No LENS tools in WITHOUT | `--disallowed-tools` blocks all 4 LENS MCP tools in WITHOUT only |
| No sub-agents | `--disallowed-tools Task` prevents output loss |
| Fresh session | Pipe mode = no conversation history |
| Isolated environment | `( ... )` subshell scopes all env vars |
| WITHOUT has zero LENS artifacts | No LENS tools, no injected context, prompt is task-only |
| WITH has LENS MCP tools | Available via `--mcp-config lens-mcp.json`, agent calls on-demand |

### Cross-Task Parallelism

Multiple tasks may run simultaneously (separate processes), but the two conditions within a single task always run sequentially.

---

## 6. Metrics

### Tool Calls & Files Read: Self-Report

`claude -p` pipe mode outputs only the agent's final response — tool invocations are not visible in the text. Self-report is the **only available source** for tool call and file read counts.

The prompt suffix asks the agent to report: (1) total tool calls, (2) LENS calls, (3) files read count, (4) files used (paths), (5) key findings.

### Duration: Machine-Derived

Wall-clock seconds from `timing.csv` (recorded by `run.sh`). Reliable and objective.

### Scoring

Each task output is scored against its judge criteria:

| Verdict | Points |
|---------|--------|
| **Met** — explicitly stated in output | 1.0 |
| **Partial** — alluded to but vague/incomplete | 0.5 |
| **Missed** — not mentioned or wrong | 0.0 |

**Score** = points / total criteria x 100%, rounded to nearest 5%.

A task is **complete** if score >= 80%.

### Quasi-Blinded Judging

Both condition outputs for a task are scored side-by-side. Criteria are applied mechanically — a fact is either stated or it isn't.

---

## 7. Gate Signals

| # | Signal | GO threshold | NO-GO threshold | Source |
|---|--------|-------------|----------------|--------|
| 1 | Top-3 hit rate (eval harness) | >80% | <60% | `lens eval` — already measured at 95% |
| 2 | Score delta when LENS adopted | >+5pp | <0pp | Tasks where agent called LENS |
| 3 | Completion rate | WITH >80% AND WITH > WITHOUT | WITH <= WITHOUT | All tasks |
| 4 | LENS adoption rate | Informational | Informational | All WITH tasks |

### Signal Definitions

**Signal 1** — Already validated. LENS returns the correct files in top-3 results 95% of the time. Not re-measured.

**Signal 2** — When the agent chose to call LENS MCP tools, did it score higher? `delta = avg_score(WITH, adopted) - avg_score(WITHOUT, same tasks)`. Computed from tasks where LENS calls > 0 in the WITH condition. If the agent never adopts LENS, this signal is unmeasurable — treat as NO-GO (product not useful if agents don't use it).

**Signal 3** — Two conditions must BOTH hold: (a) WITH completion rate >80%, AND (b) WITH completion rate strictly greater than WITHOUT. If WITH = WITHOUT, it's NO-GO (LENS adds complexity without improvement).

**Signal 4** — Informational only, not a gate signal. Adoption rate shows whether agents find LENS useful enough to call. Low adoption on large repos = tool description or UX problem. Low adoption on small repos = expected (grep is sufficient).

---

## 8. Decision Rule

| Outcome | Condition | Action |
|---------|-----------|--------|
| **GO** | Signals 1, 2, and 3 all pass | Proceed to Phase 4 (routing) + Phase 5 (weight tuning) |
| **PARTIAL** | Signal 1 passes + at most 1 of {2, 3} in NO-GO range | Fix failing signals, re-run affected tasks |
| **NO-GO** | 2+ of Signals 1-3 in NO-GO range | Pivot or stop. Document findings. |

---

## 9. Artifacts Produced

Each run produces:

```
bench/<timestamp>/
  protocol.md       <- frozen snapshot of protocol used
  scenarios.md      <- generated task definitions + judge criteria
  runs/
    <task_id>-with.md       <- full agent output (WITH condition)
    <task_id>-without.md    <- full agent output (WITHOUT condition)
    timing.csv              <- mixed rows: model_mapping header + task_id,condition,duration_s,attempt
  results.md        <- scored results + summary tables + cross-reference analysis + gate signals
```

All raw outputs are preserved for audit.

---

## 10. Discard Rules

A run is discarded and re-run if:
- Output is empty after 3 retries
- Agent crashes mid-run
- Model version changes between conditions
- LENS MCP server failed to start during WITH condition (check for MCP connection errors in output)

If >4 runs are discarded, investigate root cause before continuing.

---

## 11. Model Scope Caveat

Results are valid **only for glm-5**. A GO verdict does not automatically generalize to Claude Opus, Sonnet, or other models. If the decision needs to justify rollout for a different model, a supplementary gate run on that target model is required.

---

## 12. Execution Steps

1. Ensure both repos are indexed (`lens index`) — the WITH condition spawns a stdio MCP server per run, no daemon needed
2. Run `/benchmark -n 12 /Volumes/Drive/__x/RLM` — 12 scenarios, LENS repo
3. Run `/benchmark -n 12 /Volumes/Drive/__x/Pinnacle` — 12 scenarios, Pinnacle repo
4. Aggregate results from both runs (24 tasks, 48 runs)
5. Compute gate signals (1-3 decisive, 4 informational)
6. Record GO / PARTIAL / NO-GO verdict
7. Update `docs/ROADMAP.md`
