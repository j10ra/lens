---
phase: 01-core-daemon-mcp
plan: "04"
type: execute
wave: 3
depends_on: ["01-02", "01-03"]
files_modified:
  - apps/daemon/src/mcp.ts
autonomous: false
requirements:
  - DAEM-02

must_haves:
  truths:
    - "Claude Code (or Cursor) auto-discovers the MCP server when pointed at the daemon"
    - "The lens_grep tool is invoked at least once during a benchmark session against an unfamiliar repo"
    - "Tool adoption rate is above 0/N across 3+ benchmark repos"
    - "If adoption rate is below 100%, tool description and parameter schema are iterated until adoption improves"
  artifacts:
    - path: "apps/daemon/src/mcp.ts"
      provides: "Final tool description and schema after iteration"
      contains: "lens_grep"
  key_links:
    - from: "Claude Code / Cursor agent"
      to: "apps/daemon/src/mcp.ts"
      via: "MCP stdio auto-discovery from .mcp.json or claude_desktop_config.json"
      pattern: "lens_grep"
---

<objective>
Validate MCP tool adoption empirically: configure Claude Code (or Cursor) to use the LENS daemon as an MCP server, run benchmark sessions against 3+ unfamiliar repos, observe whether the tool is called, iterate tool description if not adopted.

Purpose: v1 had 0/9 adoption. This is the phase gate. Do NOT advance to Phase 2 until at least 1 adoption is confirmed. Adoption here means the agent autonomously calls `lens_grep` during a real coding task without being told to — not prompted.
Output: Revised `apps/daemon/src/mcp.ts` with empirically validated tool design. Gate decision recorded in SUMMARY.
</objective>

<execution_context>
@/Users/jalipalo/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jalipalo/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-core-daemon-mcp/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: MCP server registration — configure daemon as MCP provider for Claude Code</name>
  <files>
    apps/daemon/src/mcp.ts
  </files>
  <action>
Verify and harden the MCP server registration so Claude Code can auto-discover it.

Step 1 — Create `.mcp.json` in the repo root for Claude Code auto-discovery:
```json
{
  "mcpServers": {
    "lens": {
      "command": "node",
      "args": ["apps/daemon/dist/index.js"],
      "env": {
        "LENS_DATA_DIR": "${HOME}/.lens"
      }
    }
  }
}
```

Step 2 — Rebuild daemon if any changes were made since plan 02:
```bash
pnpm --filter @lens/daemon build
```

Step 3 — Verify the MCP server starts cleanly and lists tools:
```bash
# Use the MCP inspector to verify tool registration
# Run daemon in MCP mode and send a tools/list request manually:
echo '{"jsonrpc":"2.0","id":1,"method":"tools/list","params":{}}' | node apps/daemon/dist/index.js
# Expected output on stdout: JSON response containing lens_grep in tools array
# Stderr should show: [daemon] HTTP server listening on :4111
```

Step 4 — Verify tool schema is complete and parseable. The `lens_grep` tool must have:
- `name`: `"lens_grep"`
- `description`: starts with an action verb, 1-2 sentences max
- `inputSchema`: `repoPath` (string, required), `query` (string, required), `limit` (integer, optional)
- Each parameter has a `.describe()` with concrete examples

If the manual tools/list JSON is malformed or missing the tool, fix `apps/daemon/src/mcp.ts` before proceeding to the benchmark checkpoint.
  </action>
  <verify>
    `.mcp.json` exists at repo root with correct daemon command.
    `echo '{"jsonrpc":"2.0","id":1,"method":"tools/list","params":{}}' | node apps/daemon/dist/index.js` outputs a JSON response on stdout containing `"name":"lens_grep"`.
    Tool description in the JSON response starts with a verb and is under 200 characters.
    All three parameters (`repoPath`, `query`, `limit`) appear in the `inputSchema` with descriptions.
  </verify>
  <done>MCP server is registered, tool appears in tools/list JSON, .mcp.json exists for agent auto-discovery.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
    MCP daemon with lens_grep tool. .mcp.json configured for Claude Code auto-discovery.
    The tool accepts repoPath + query, returns a structured JSON response (currently a stub indicating the engine is not yet indexed).
  </what-built>
  <how-to-verify>
    Run 3+ benchmark sessions. Each session: open Claude Code in an UNFAMILIAR repo (not this one), give a realistic coding task that requires understanding the codebase structure (e.g., "Where is the auth middleware applied? What files import it?"), and observe whether the agent calls lens_grep without being prompted.

    **Benchmark Protocol:**
    1. Start daemon: `LENS_MCP=false node apps/daemon/dist/index.js &amp;` (HTTP only for now is fine — MCP auto-starts via .mcp.json when Claude Code spawns it)
    2. Open Claude Code in an unfamiliar repo (suggestion: a popular OSS repo you haven't worked in)
    3. Ask a structural question: "Find all places where the database connection is used and what modules depend on it"
    4. Watch the tool use panel — did Claude call lens_grep?
    5. Record: repo name, task given, tool called (yes/no), if no — what tools did Claude use instead?

    **Iteration criteria:**
    - If adoption rate is 0/3: the tool description or schema is the problem. Try these changes in order:
      a. Make the description more imperative: "Use this tool to find where any symbol, function, or concept lives in the codebase graph"
      b. Rename to something more obvious: `search_codebase` or `find_code_context`
      c. Add `annotations.audience: ["agent"]` to tool registration if SDK supports it
    - After each change: rebuild daemon, re-run 1-2 benchmark sessions
    - Continue until at least 1/3 adoption is achieved

    **Adoption confirmed when:** Tool is called at least once in N sessions without user instruction. Record final N and adoption count.
  </how-to-verify>
  <resume-signal>
    Type "adopted: X/N" (e.g., "adopted: 2/3") when done. If you changed the tool description, paste the final description that achieved adoption. If adoption could not be confirmed after 3+ iterations, type "blocked: [description of what was tried]" and we will discuss before Phase 2.
  </resume-signal>
</task>

</tasks>

<verification>
Gate criteria (ALL must be true before Phase 2 begins):

1. `lens_grep` appears in Claude Code tool list without manual configuration
2. At least 1 unprompted tool invocation recorded across 3+ benchmark sessions
3. Final tool description and schema are committed to `apps/daemon/src/mcp.ts`
4. Adoption data (N sessions, M calls) recorded in 01-04-SUMMARY.md

If gate FAILS (0/N adoption after multiple iterations):
- Do not start Phase 2
- Review tool naming, description, schema verbosity
- Consider: is the tool being discovered at all? Check Claude Code MCP panel.
- Escalate: discuss with user before proceeding
</verification>

<success_criteria>
Gate PASSES when:
- Adoption rate > 0/N (at least 1 call in N sessions, N >= 3)
- Tool description is verb-first, under 200 chars, no parameter detail in description body
- .mcp.json committed and working
- 01-04-SUMMARY.md records: sessions run, adoption count, final tool description, any iterations made
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-daemon-mcp/01-04-SUMMARY.md` with:
- GATE RESULT: PASSED or FAILED
- Sessions run: N
- Tool invocations: M
- Adoption rate: M/N
- Final tool description (the exact string in mcp.ts)
- Iterations made (if any): what was tried, what worked
- Decision: "Phase 2 may proceed" or "Phase 2 blocked pending [action]"
</output>