---
phase: 02-intelligence-engine
plan: "03"
type: execute
wave: 3
depends_on: ["02-01", "02-02"]
files_modified:
  - packages/engine/src/index/engine.ts
  - packages/engine/src/grep/scorer.ts
  - packages/engine/src/grep/structural.ts
autonomous: true
requirements: [ENGN-02, ENGN-05, ENGN-06]

must_haves:
  truths:
    - "runIndex() orchestrates: discovery → chunking → metadata → import graph → git analysis in sequence"
    - "runIndex() uses in-memory mutex to prevent concurrent indexing of the same repo"
    - "runIndex() skips indexing if HEAD commit matches last_indexed_commit (unless force=true)"
    - "interpretQuery() computes TF-IDF scores over fileMetadata with per-field weights"
    - "Composite score includes indegree boost (log2), hotness boost (recent_count), and hub dampening (exports>5)"
    - "getIndegrees() returns import counts per file; getCochangePartners() returns top co-change partners"
    - "Hub files are identified by indegree >= 5 threshold"
  artifacts:
    - path: "packages/engine/src/index/engine.ts"
      provides: "runIndex() orchestrator with mutex"
      exports: ["runIndex"]
    - path: "packages/engine/src/grep/scorer.ts"
      provides: "interpretQuery() — TF-IDF + structural boosters"
      exports: ["interpretQuery"]
    - path: "packages/engine/src/grep/structural.ts"
      provides: "getIndegrees(), getReverseImports(), getCochangePartners()"
      exports: ["getIndegrees", "getReverseImports", "getCochangePartners"]
  key_links:
    - from: "packages/engine/src/index/engine.ts"
      to: "packages/engine/src/index/discovery.ts"
      via: "fullScan() / diffScan() calls"
      pattern: "fullScan|diffScan"
    - from: "packages/engine/src/index/engine.ts"
      to: "packages/engine/src/index/import-graph.ts"
      via: "buildAndPersistImportGraph() call"
      pattern: "buildAndPersistImportGraph"
    - from: "packages/engine/src/index/engine.ts"
      to: "packages/engine/src/index/git-analysis.ts"
      via: "analyzeGitHistory() call"
      pattern: "analyzeGitHistory"
    - from: "packages/engine/src/grep/scorer.ts"
      to: "packages/engine/src/grep/structural.ts"
      via: "getIndegrees() for scoring boosters"
      pattern: "getIndegrees"
---

<objective>
Index orchestrator and composite scoring engine — the two core algorithms that tie all engine signals together.

Purpose: `runIndex()` orchestrates the full indexing pipeline (discovery → chunk → metadata → import graph → git analysis) with mutex locking. The scorer combines TF-IDF over metadata fields with structural boosters (indegree, co-change hotness, hub dampening) into a single composite score per file.
Output: `engine.ts` (index orchestrator), `scorer.ts` (composite scoring), `structural.ts` (graph query helpers).
</objective>

<execution_context>
@/Users/jalipalo/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jalipalo/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-intelligence-engine/02-RESEARCH.md
@.planning/phases/02-intelligence-engine/02-01-SUMMARY.md
@.planning/phases/02-intelligence-engine/02-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Index orchestrator with mutex</name>
  <files>
    packages/engine/src/index/engine.ts
  </files>
  <action>
**engine.ts** — Port from v1 (`git show v1-archive:packages/engine/src/index/engine.ts`):

**In-memory mutex** (15 lines, from research Pattern 4):
```typescript
const locks = new Map<string, Promise<void>>();
async function withLock<T>(key: string, fn: () => Promise<T>): Promise<T> {
  while (locks.has(key)) await locks.get(key);
  let resolve!: () => void;
  const p = new Promise<void>((r) => { resolve = r; });
  locks.set(key, p);
  try { return await fn(); }
  finally { locks.delete(key); resolve(); }
}
```

**runIndex** — Wrap in `lensFn("engine.runIndex", ...)`. This IS a top-level exported async function.

Algorithm:
1. Get repo by id from DB. Throw if not found.
2. Get HEAD commit via `getHeadCommit(repo.root_path)`.
3. Early exit if `!force && repo.last_indexed_commit === headCommit` → return `{ skipped: true, ... }`.
4. Set repo status to "indexing": `repoQueries.setIndexing(db, repoId)`.
5. **Discovery**: `const isFullScan = force || !repo.last_indexed_commit`. Call `fullScan()` or `diffScan()` accordingly.
6. **Chunking + storage**: For each discovered file:
   - Read file content via `readFile(join(repo.root_path, file.path), "utf-8")`. Wrap in try/catch (file may have been deleted between discovery and read).
   - Call `chunkFile(content, file.path)`.
   - Upsert chunks: `chunkQueries.upsertChunks(db, repoId, file.path, chunks.map(c => ({ ...c, language: file.language, lastSeenCommit: headCommit })))`.
7. **Metadata extraction**: `extractAndPersistMetadata(db, repoId)`.
8. **Import graph**: `buildAndPersistImportGraph(db, repoId)`.
9. **Git analysis**: `await analyzeGitHistory(db, repoId, repo.root_path, repo.last_git_analysis_commit)`.
10. Update repo state: `repoQueries.updateIndexState(db, repoId, headCommit, "ready")`.
11. Return `IndexResult` with `files_scanned`, `chunks_created`, `duration_ms`, `skipped: false`.

**IndexResult type**: `{ files_scanned: number, chunks_created: number, duration_ms: number, skipped: boolean }`.

The entire indexing call is wrapped in `withLock(repoId, ...)` to prevent concurrent indexing of the same repo. Different repos can index concurrently.

Import `readFile` from `node:fs/promises`. Import all pipeline functions from sibling modules.
  </action>
  <verify>
    `pnpm --filter @lens/engine build` succeeds.
    `pnpm --filter @lens/engine typecheck` exits 0.
    `grep -r "withLock" packages/engine/src/index/engine.ts` confirms mutex.
    `grep -r "lensFn" packages/engine/src/index/engine.ts` confirms lensFn wrapping.
    `grep -r "fullScan\|diffScan\|extractAndPersistMetadata\|buildAndPersistImportGraph\|analyzeGitHistory" packages/engine/src/index/engine.ts` confirms pipeline steps.
  </verify>
  <done>
    runIndex() orchestrates full pipeline: discovery → chunk → metadata → import graph → git analysis. Mutex prevents concurrent indexing of same repo. Early exit on unchanged HEAD. Index status transitions: pending → indexing → ready.
  </done>
</task>

<task type="auto">
  <name>Task 2: TF-IDF scorer, structural queries, hub detection</name>
  <files>
    packages/engine/src/grep/scorer.ts
    packages/engine/src/grep/structural.ts
  </files>
  <action>
**structural.ts** — DB query helpers for graph-based signals:

- `getIndegrees(db: Db, repoId: string): Map<string, number>` — Count incoming edges per target_path in fileImports table. Returns `Map<filePath, indegreeCount>`. Use SQL: `SELECT target_path, COUNT(*) as cnt FROM file_imports WHERE repo_id = ? GROUP BY target_path`.
- `getReverseImports(db: Db, repoId: string, targetPath: string): string[]` — Return all source_paths that import the given target_path. Use `importQueries.getImporters()`.
- `getCochangePartners(db: Db, repoId: string, filePath: string, limit?: number): Array<{ path: string, count: number }>` — Return top co-change partners for a file. Query fileCochanges where `path_a = filePath OR path_b = filePath`, return the OTHER path and count, ordered by count desc, limited.
- `getFileStats(db: Db, repoId: string, filePath: string): { commitCount: number, recentCount: number } | null` — Get commit stats for a file.

All synchronous (better-sqlite3). Do NOT wrap in lensFn — these are internal query helpers called from scorer.

**scorer.ts** — Port from v1 (`git show v1-archive:packages/engine/src/context/query-interpreter.ts`):

- `interpretQuery(db: Db, repoId: string, terms: string[], limit: number): ScoredFile[]` — The core scoring algorithm.

  Algorithm:
  1. Load all fileMetadata for repo: `metadataQueries.getAllForRepo(db, repoId)`.
  2. Load indegrees: `getIndegrees(db, repoId)`.
  3. Compute document frequencies (DF): for each term, count how many files match it in any metadata field. `N` = total files.
  4. **IDF formula**: `IDF(term) = Math.min(10, Math.max(1, Math.log(N / df)))` where df = files containing term.
  5. **Per-file scoring**: For each file, for each term:
     - Check presence in each metadata field. A "match" means the lowercased field content contains the lowercased term.
     - Field weights: fileName tokens → 4x, dirPath tokens → 2x, exports → 2.5x (exact token match on camelCase-decomposed), docstring → 1x, sections → 1x, internals → 1.5x.
     - `fileScore += IDF(term) * fieldWeight` for each matching field.
  6. **Structural boosters** (applied after TF-IDF base):
     - Hotness: `const stats = getFileStats(db, repoId, file.path)`. If `stats.recentCount > 0`: `score += Math.min(5, stats.recentCount * 0.5)`.
     - Indegree: If `indegree >= 3`: `score *= 1 + Math.log2(indegree) * 0.1`.
     - Hub dampening: Parse exports from metadata. If `exports.length > 5`: `score *= 1 / (1 + Math.log2(exports.length / 5) * 0.3)`. This prevents catch-all barrel files from dominating.
     - Multi-term coverage: If `matchedTermCount > 1`: `const coverage = matchedTermCount / terms.length; score *= 1 + coverage * coverage`.
  7. Sort by score descending, take top `limit`.
  8. Return `ScoredFile[]` where each entry has: `{ path, score, language, matchedTerms: string[], isHub: boolean, hubScore: number }`.
     - `isHub = indegree >= HUB_THRESHOLD` (HUB_THRESHOLD = 5).
     - `hubScore = computeHubScore(indegree, maxIndegree)` where `maxIndegree` is the max across all files.
     - `computeHubScore(indegree, maxIndegree) = maxIndegree === 0 ? 0 : Math.min(1, indegree / maxIndegree)`.

  Helper: `decomposeTokens(name: string): string[]` — Split camelCase/PascalCase/snake_case into lowercase tokens. Used for matching exports against query terms.

  This is synchronous (all DB calls are sync via better-sqlite3). Do NOT wrap in lensFn — it's called from grepRepo() which will be lensFn-wrapped in Plan 02-04.
  </action>
  <verify>
    `pnpm --filter @lens/engine build` succeeds.
    `pnpm --filter @lens/engine typecheck` exits 0.
    `grep -r "IDF\|fieldWeight\|indegree\|recentCount\|hubScore" packages/engine/src/grep/scorer.ts` confirms scoring formula components.
    `grep -r "HUB_THRESHOLD" packages/engine/src/grep/scorer.ts` confirms hub detection threshold.
    `grep -r "getIndegrees\|getReverseImports\|getCochangePartners" packages/engine/src/grep/structural.ts` confirms structural queries.
  </verify>
  <done>
    TF-IDF scoring over metadata fields with per-field weights (ENGN-02). Hub detection via indegree >= 5 threshold (ENGN-05). Composite scoring combines TF-IDF base + indegree boost + hotness boost + hub dampening + multi-term coverage (ENGN-06). Structural query helpers return indegrees, reverse imports, and co-change partners.
  </done>
</task>

</tasks>

<verification>
- `pnpm --filter @lens/engine build` exits 0
- `pnpm --filter @lens/engine typecheck` exits 0
- `grep -r "withLock" packages/engine/src/index/engine.ts` shows mutex
- `grep -r "lensFn" packages/engine/src/index/engine.ts` shows wrapping
- `grep -rn "HUB_THRESHOLD\|IDF\|fieldWeight" packages/engine/src/grep/` shows scoring components
- `grep -r "getIndegrees\|getReverseImports\|getCochangePartners" packages/engine/src/grep/structural.ts` shows structural queries
</verification>

<success_criteria>
- runIndex() orchestrates full pipeline with proper ordering
- Mutex prevents concurrent same-repo indexing
- TF-IDF scoring uses per-field weights (fileName 4x, exports 2.5x, etc.)
- Structural boosters: indegree boost, hotness, hub dampening, multi-term coverage
- Hub threshold is 5; hubScore normalized to 0-1
- All scoring is synchronous via better-sqlite3
</success_criteria>

<output>
After completion, create `.planning/phases/02-intelligence-engine/02-03-SUMMARY.md`
</output>
